{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4556a7bf-7a7b-4877-864d-c8448d753e71",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1e23aa8-3a39-465e-87bf-384eac03f1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import csv\n",
    "np.random.seed(42)\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8aa344-11fa-4c8d-8dcf-99d7cfa26a0f",
   "metadata": {},
   "source": [
    "## Downsample and Filtering\n",
    "Trim down the dataset to a tenth of the size so that the next processing steps can be done quicker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69dd1e8d-c4f8-48c7-a188-3f1d95221fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(chunk.dtypes)\n",
    "# s = \"\"\"...\"\"\".split()\n",
    "# for i in range(0, len(s), 2):\n",
    "#     print(f'\"{s[i]}\": \"{s[i+1]}\",')\n",
    "\n",
    "# Define a schema so that the datatypes are the same across the chunks\n",
    "schema = {\n",
    "    \"recommendationid\": \"Int64\",\n",
    "    \"appid\": \"Int64\",\n",
    "    \"game\": \"object\",\n",
    "    \"author_steamid\": \"Int64\",\n",
    "    \"author_num_games_owned\": \"Int64\",\n",
    "    \"author_num_reviews\": \"Int64\",\n",
    "    \"author_playtime_forever\": \"Int64\",\n",
    "    \"author_playtime_last_two_weeks\": \"Int64\",\n",
    "    \"author_playtime_at_review\": \"Int64\",\n",
    "    \"author_last_played\": \"Int64\",\n",
    "    \"language\": \"object\",\n",
    "    \"review\": \"object\",\n",
    "    \"timestamp_created\": \"Int64\",\n",
    "    \"timestamp_updated\": \"Int64\",\n",
    "    \"voted_up\": \"Int64\",\n",
    "    \"votes_up\": \"Int64\",\n",
    "    \"votes_funny\": \"Int64\",\n",
    "    \"weighted_vote_score\": \"float64\",\n",
    "    \"comment_count\": \"Int64\",\n",
    "    \"steam_purchase\": \"Int64\",\n",
    "    \"received_for_free\": \"Int64\",\n",
    "    \"written_during_early_access\": \"Int64\",\n",
    "    \"hidden_in_steam_china\": \"Int64\",\n",
    "    \"steam_china_location\": \"object\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90e553d7-18eb-43bd-b0bc-cba6bc93fe85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1138it [18:55,  1.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# Trim down the dataset to a tenth to make next steps quicker\n",
    "all_path = \"steam_dataset/all_reviews.csv\"\n",
    "subset_path = \"steam_dataset/subset_reviews.csv\"\n",
    "\n",
    "process = lambda x: x.sample(frac=0.1, random_state=np.random.randint(0, 100))\n",
    "all_reader = pd.read_csv(all_path, chunksize=100_000, dtype=schema)\n",
    "    \n",
    "# First write will create/truncate file and write the header\n",
    "chunk = process(next(all_reader))\n",
    "chunk.to_csv(subset_path, mode='w', header=True, index=False, quoting=csv.QUOTE_NONNUMERIC)\n",
    "\n",
    "# about 20 mins\n",
    "for c in tqdm(all_reader):\n",
    "    # next writes will append to file and not write the header\n",
    "    chunk = process(c)\n",
    "    chunk.to_csv(subset_path, mode='a', header=False, index=False, quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e405caf-8605-421d-8be4-b1755b1cc178",
   "metadata": {},
   "source": [
    "Filter out non-English reviews to create an English only dataset. This will make it easier to perform analysis later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dd2bfbc-4aae-409c-9be6-3eebf2b1c8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113it [02:36,  1.38s/it]\n"
     ]
    }
   ],
   "source": [
    "all_path = \"steam_dataset/subset_reviews.csv\"\n",
    "eng_path = \"steam_dataset/eng_reviews.csv\"\n",
    "\n",
    "process = lambda x: x[x[\"language\"] == \"english\"]\n",
    "all_reader = pd.read_csv(all_path, chunksize=100_000, dtype=schema)\n",
    "\n",
    "# First write will create/truncate file and write the header\n",
    "chunk = process(next(all_reader))\n",
    "chunk.to_csv(eng_path, mode='w', header=True, index=False, quoting=csv.QUOTE_NONNUMERIC)\n",
    "\n",
    "for c in tqdm(all_reader):\n",
    "    # next writes will append to file and not write the header\n",
    "    chunk = process(c)\n",
    "    chunk.to_csv(eng_path, mode='a', header=False, index=False, quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e646d12a-a27b-4c92-92bb-aa02ee82ecf6",
   "metadata": {},
   "source": [
    "Trim down the dataset even more until we have around 2 million for the medium dataset and 20K for the small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a7e1eb3-828c-469e-bd2e-f8fbb52b55b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of lines using 'wc -l' in the terminal\n",
    "# the previous cells should be deterministic due to our seed\n",
    "# we also want our small set to be a subset of the medium set\n",
    "n = 5155338\n",
    "med_n = 2_000_000\n",
    "sml_n = 20_000\n",
    "\n",
    "med_prob = med_n / n\n",
    "# sml_prob = sml_n / n\n",
    "sml_prob = sml_n / med_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb959659-8e1e-47fa-ad6c-d6f00c5d2f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [01:04,  1.26s/it]\n"
     ]
    }
   ],
   "source": [
    "all_path = \"steam_dataset/eng_reviews.csv\"\n",
    "med_path = \"steam_dataset/med_sample.csv\"\n",
    "sml_path = \"steam_dataset/sml_sample.csv\"\n",
    "\n",
    "med_process = lambda x: x.sample(frac=med_prob, random_state=np.random.randint(0, 100))\n",
    "sml_process = lambda x: x.sample(frac=sml_prob, random_state=np.random.randint(0, 100))\n",
    "all_reader = pd.read_csv(all_path, chunksize=100_000, dtype=schema)\n",
    "\n",
    "# First write will create/truncate file and write the header\n",
    "med_chunk = med_process(next(all_reader))\n",
    "sml_chunk = sml_process(med_chunk)\n",
    "\n",
    "med_chunk.to_csv(med_path, mode='w', header=True, index=False, quoting=csv.QUOTE_NONNUMERIC)\n",
    "sml_chunk.to_csv(sml_path, mode='w', header=True, index=False, quoting=csv.QUOTE_NONNUMERIC)\n",
    "\n",
    "for chunk in tqdm(all_reader):\n",
    "    # next writes will append to file and not write the header\n",
    "    med_chunk = med_process(chunk)\n",
    "    sml_chunk = sml_process(med_chunk)\n",
    "    \n",
    "    med_chunk.to_csv(med_path, mode='a', header=False, index=False, quoting=csv.QUOTE_NONNUMERIC)\n",
    "    sml_chunk.to_csv(sml_path, mode='a', header=False, index=False, quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f0b5ce-4fbc-4d21-a3e8-4b29f30ef68d",
   "metadata": {},
   "source": [
    "# Convert to Parquet \n",
    "Sanity check for our new datasets and filter out some columns, then convert to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f9aa11d-c509-42d9-b32c-25aa1d294e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20003\n",
      "recommendationid                    int64\n",
      "appid                               int64\n",
      "game                               object\n",
      "author_steamid                      int64\n",
      "author_num_games_owned              int64\n",
      "author_num_reviews                  int64\n",
      "author_playtime_forever             int64\n",
      "author_playtime_last_two_weeks      int64\n",
      "author_playtime_at_review           int64\n",
      "author_last_played                  int64\n",
      "language                           object\n",
      "review                             object\n",
      "timestamp_created                   int64\n",
      "timestamp_updated                   int64\n",
      "voted_up                            int64\n",
      "votes_up                            int64\n",
      "votes_funny                         int64\n",
      "weighted_vote_score               float64\n",
      "comment_count                       int64\n",
      "steam_purchase                      int64\n",
      "received_for_free                   int64\n",
      "written_during_early_access         int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "sml_path = \"steam_dataset/sml_sample.csv\"\n",
    "sml = pd.read_csv(sml_path)\n",
    "sml = sml.drop(['hidden_in_steam_china', 'steam_china_location'], axis='columns', errors='ignore')\n",
    "\n",
    "print(len(sml))\n",
    "# sml.head()\n",
    "# sml.columns\n",
    "print(sml.dtypes)\n",
    "\n",
    "sml.to_csv(sml_path, index=False, quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "605e9308-08a0-4cf2-aa34-7a041fb8408e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000013\n",
      "recommendationid                    int64\n",
      "appid                               int64\n",
      "game                               object\n",
      "author_steamid                      int64\n",
      "author_num_games_owned              int64\n",
      "author_num_reviews                  int64\n",
      "author_playtime_forever             int64\n",
      "author_playtime_last_two_weeks      int64\n",
      "author_playtime_at_review           int64\n",
      "author_last_played                  int64\n",
      "language                           object\n",
      "review                             object\n",
      "timestamp_created                   int64\n",
      "timestamp_updated                   int64\n",
      "voted_up                            int64\n",
      "votes_up                            int64\n",
      "votes_funny                         int64\n",
      "weighted_vote_score               float64\n",
      "comment_count                       int64\n",
      "steam_purchase                      int64\n",
      "received_for_free                   int64\n",
      "written_during_early_access         int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "med_path = \"steam_dataset/med_sample.csv\"\n",
    "med = pd.read_csv(med_path)\n",
    "med = med.drop(['hidden_in_steam_china', 'steam_china_location'], axis='columns', errors='ignore')\n",
    "\n",
    "print(len(med))\n",
    "# med.head()\n",
    "# med.columns\n",
    "print(med.dtypes)\n",
    "\n",
    "med.to_csv(med_path, index=False, quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b444a87-8c58-4f82-bd06-018a84fdcce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "med_path_parquet = 'steam_dataset/med_sample.parquet'\n",
    "sml_path_parquet = 'steam_dataset/sml_sample.parquet'\n",
    "\n",
    "table = pa.Table.from_pandas(pd.read_csv(med_path))\n",
    "pq.write_table(table, med_path_parquet)\n",
    "\n",
    "table = pa.Table.from_pandas(pd.read_csv(sml_path))\n",
    "pq.write_table(table, sml_path_parquet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Big Data",
   "language": "python",
   "name": "big_data"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
